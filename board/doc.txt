from azure.storage.blob import BlobServiceClient

# Initialize Azure Blob Service Client
connection_string = "<your-connection-string>"
container_name = "<your-container-name>"
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client(container_name)

# Upload files to Azure Storage
for file, file_path in target_files:
    blob_client = container_client.get_blob_client(file)
    with open(file_path, "rb") as data:
        blob_client.upload_blob(data)


from pyspark.sql.functions import when

# Load the metadata table into a DataFrame
metadata_df = spark.read.table("vfs_raw.file_metadata")

# Update upload_status and upload_on for all files
metadata_df = metadata_df.withColumn("upload_status", when(metadata_df.file_name.isin([file[0] for file in target_files]), "Uploaded")) \
                         .withColumn("upload_on", when(metadata_df.file_name.isin([file[0] for file in target_files]), current_timestamp()))

# Save the updated metadata back to the Delta table
metadata_df.write.format("delta").mode("overwrite").saveAsTable("vfs_raw.file_metadata")
