-- Create database if it doesn't exist
CREATE DATABASE IF NOT EXISTS vfs_raw;

-- Create the file_metadata table in the vfs_raw database with a custom location
CREATE TABLE IF NOT EXISTS vfs_raw.file_metadata (
    file_name STRING COMMENT 'Name of the file',
    file_relative_path STRING COMMENT 'Relative path of the file in the repository',
    file_extension STRING COMMENT 'File extension (e.g., md, txt, pdf)',
    category STRING COMMENT 'Category or type of the file (e.g., research_findings)',
    git_last_commit_date TIMESTAMP COMMENT 'Timestamp of the last Git commit for the file',
    git_blob_hash STRING COMMENT 'Git blob hash (unique identifier for the file content)',
    upload_status STRING COMMENT 'Status of the file upload (e.g., pending, needs_reupload, uploaded)',
    upload_on TIMESTAMP COMMENT 'Timestamp when the file was uploaded to Azure',
    blob_url STRING COMMENT 'URL of the file in Azure Blob Storage',
    error_message STRING COMMENT 'Error message if the file upload failed',
    etl_created_at TIMESTAMP COMMENT 'Timestamp when the record was created in the Delta table',
    etl_updated_at TIMESTAMP COMMENT 'Timestamp when the record was last updated in the Delta table'
)
USING delta
LOCATION 'vfs/raw/metadata';


from pyspark.sql import SparkSession
from pyspark.sql.functions import date_format, current_timestamp
from typing import List, Dict

from pyspark.sql import SparkSession
from pyspark.sql.functions import date_format, current_timestamp
from typing import List, Dict

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import date_format, current_timestamp
from typing import List, Dict

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import to_timestamp, date_format, current_timestamp
from typing import List, Dict

def first_time_load_metadata_to_delta_table(metadata_list: List[Dict]):
    """
    Performs the first-time load of file metadata into the vfs_raw.file_metadata Delta table.
    Uses overwrite mode to ensure the table is fresh and doesn't contain duplicates.

    Args:
        metadata_list: List of dictionaries, where each dictionary contains metadata for a file.
                      Each dictionary must have keys: file_name, file_relative_path, file_extension,
                      category, git_last_commit_date, git_blob_hash, upload_status, upload_on, blob_url, error_message.
    """
    spark = SparkSession.builder.appName("FirstTimeLoadMetadata").getOrCreate()
    
    # Define the schema explicitly
    schema = StructType([
        StructField("file_name", StringType(), nullable=False),
        StructField("file_relative_path", StringType(), nullable=False),
        StructField("file_extension", StringType(), nullable=False),
        StructField("category", StringType(), nullable=False),
        StructField("git_last_commit_date", StringType(), nullable=False),  # Temporarily as StringType
        StructField("git_blob_hash", StringType(), nullable=False),
        StructField("upload_status", StringType(), nullable=False),
        StructField("upload_on", StringType(), nullable=True),  # Temporarily as StringType
        StructField("blob_url", StringType(), nullable=True),
        StructField("error_message", StringType(), nullable=True)
    ])
    
    # Convert metadata list to DataFrame with explicit schema
    initial_df = spark.createDataFrame(metadata_list, schema=schema)
    
    # Convert string timestamps to TimestampType
    initial_df = initial_df.withColumn(
        "git_last_commit_date",
        to_timestamp("git_last_commit_date", "yyyy-MM-dd'T'HH:mm:ssXXX")  # ISO 8601 format with timezone
    ).withColumn(
        "upload_on",
        to_timestamp("upload_on", "yyyy-MM-dd'T'HH:mm:ssXXX")  # ISO 8601 format with timezone
    )
    
    # Add etl_created_at and etl_updated_at fields with current timestamp in ISO 8601 format
    initial_df = initial_df.withColumn("etl_created_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ")) \
                           .withColumn("etl_updated_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ"))
    
    # Insert all records into the Delta table in overwrite mode
    initial_df.write.format("delta") \
        .mode("overwrite") \  # Overwrite the table to ensure it's fresh
        .saveAsTable("vfs_raw.file_metadata")
    
    print("First-time load of metadata into Delta table completed successfully.")




# Example metadata list for initial load
initial_metadata_list = [
    {
        "file_name": "research_report.md",
        "file_relative_path": "file/relative_path/research_report.md",
        "file_extension": "md",
        "category": "research_findings",
        "git_last_commit_date": "2022-09-27T18:00:00.000Z",
        "git_blob_hash": "af5626b4a114abcb82d63db7c8082c3c4756e51b",
        "upload_status": "pending",
        "upload_on": None,
        "blob_url": None,  # Initially empty
        "error_message": None
    }
]

# Perform initial load
initial_load_file_metadata(initial_metadata_list)





# Example metadata list for incremental update
incremental_metadata_list = [
    {
        "file_name": "research_report.md",  # Existing file (update)
        "file_relative_path": "file/relative_path/research_report.md",
        "file_extension": "md",
        "category": "research_findings",
        "git_last_commit_date": "2023-10-26T18:00:00.000Z",  # Newer commit date
        "git_blob_hash": "df5626b4a114abcb82d63db7c8082c3c4756e51b",  # Different hash
        "upload_status": "needs_reupload",  # Needs to be re-uploaded
        "upload_on": None,  # Reset since the file needs to be re-uploaded
        "blob_url": None,  # Reset since the file needs to be re-uploaded
        "error_message": None
    },
    {
        "file_name": "new_research_report.md",  # New file (insert)
        "file_relative_path": "file/relative_path/new_research_report.md",
        "file_extension": "md",
        "category": "research_findings",
        "git_last_commit_date": "2023-10-25T14:30:45.123Z",  # ISO format
        "git_blob_hash": "ef5626b4a114abcb82d63db7c8082c3c4756e51b",  # New hash
        "upload_status": "pending",  # New file, needs to be uploaded
        "upload_on": None,
        "blob_url": None,  # Initially empty
        "error_message": None
    }
]

# Perform incremental update
incremental_update_file_metadata(incremental_metadata_list)


################
from pyspark.sql import SparkSession
from pyspark.sql.functions import date_format, current_timestamp
from typing import List, Dict

def incremental_update_file_metadata(metadata_list: List[Dict]):
    """
    Handles incremental updates (inserts and updates) for the vfs_raw.file_metadata table.

    Args:
        metadata_list: List of dictionaries, where each dictionary contains metadata for a file.
                      Each dictionary must have keys: file_name, file_relative_path, file_extension,
                      category, git_last_commit_date, git_blob_hash, upload_status, upload_on, blob_url, error_message.
    """
    spark = SparkSession.builder.appName("IncrementalUpdateFileMetadata").getOrCreate()
    
    # Convert metadata list to DataFrame
    incremental_df = spark.createDataFrame(metadata_list)
    
    # Add ISO 8601 formatted timestamps
    incremental_df = incremental_df.withColumn("etl_created_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ")) \
                                   .withColumn("etl_updated_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ"))
    
    # Perform MERGE operation to handle inserts and updates
    incremental_df.createOrReplaceTempView("incremental_updates")
    
    spark.sql("""
    MERGE INTO vfs_raw.file_metadata AS target
    USING incremental_updates AS source
    ON target.file_relative_path = source.file_relative_path
    WHEN MATCHED AND (
        source.git_last_commit_date > target.git_last_commit_date OR
        source.git_blob_hash != target.git_blob_hash
    ) THEN
        UPDATE SET
            target.git_last_commit_date = source.git_last_commit_date,  -- Update commit date
            target.git_blob_hash = source.git_blob_hash,  -- Update blob hash
            target.upload_status = 'needs_reupload',  -- Set status to indicate re-upload is needed
            target.upload_on = NULL,  -- Reset upload_on since the file needs to be re-uploaded
            target.blob_url = NULL,  -- Reset blob_url since the file needs to be re-uploaded
            target.error_message = NULL,  -- Clear any previous error message
            target.etl_updated_at = source.etl_updated_at  -- Update the last updated timestamp
    WHEN NOT MATCHED THEN
        INSERT (
            file_name, file_relative_path, file_extension, category,
            git_last_commit_date, git_blob_hash, upload_status, upload_on,
            blob_url, error_message, etl_created_at, etl_updated_at
        )
        VALUES (
            source.file_name, source.file_relative_path, source.file_extension, source.category,
            source.git_last_commit_date, source.git_blob_hash, source.upload_status, source.upload_on,
            source.blob_url, source.error_message, source.etl_created_at, source.etl_updated_at
        )
    """)
    
    print("Incremental update completed successfully.")
