def deduplicate_by_blob_hash(spark, table_name: str) -> int:
   """
   Deduplicate records in a Delta table based on git_blob_hash.
   For records with same hash, keeps the one with latest etl_updated_at and highest id.
   
   Args:
       spark: SparkSession object
       table_name: Fully qualified table name (e.g., 'vfs_raw.file_metadata')
   
   Returns:
       Number of duplicate records removed
   """
   # Read the source table
   df = spark.table(table_name)
   
   # Create window spec and add rank
   window_spec = Window.partitionBy("git_blob_hash")\
                      .orderBy(df.etl_updated_at.desc(), df.id.desc())
   ranked_df = df.withColumn("rank", row_number().over(window_spec))
   
   # Get IDs to keep (rank = 1)
   ids_to_keep = ranked_df.filter("rank = 1")\
                         .select("id")\
                         .rdd.flatMap(lambda x: x)\
                         .collect()
   
   # Get count of duplicates for return value                      
   duplicate_count = ranked_df.filter("rank > 1").count()
   
   # Rewrite table with only non-duplicate records
   temp_view = f"temp_{table_name.replace('.', '_')}"
   df.filter(df.id.isin(ids_to_keep))\
     .createOrReplaceTempView(temp_view)
   
   spark.sql(f"""
       TRUNCATE TABLE {table_name};
       INSERT INTO {table_name}
       SELECT * FROM {temp_view};
   """)
   
   return duplicate_count
