from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import col, lit, to_timestamp
from typing import List, Dict
from delta.tables import DeltaTable
import common_utils  # For get_standarized_timestamp

# Define constant for table name at the top
FILE_METADATA_TABLE = "vfs.target_file_metadata"

# Initialize Spark session outside the function
spark = SparkSession.builder.getOrCreate()

# Define schema for target_files DataFrame
TARGET_FILES_SCHEMA = StructType([
    StructField("file_relative_path", StringType(), False),
    StructField("git_blob_hash", StringType(), False),
    StructField("category", StringType(), False),
    StructField("file_name", StringType(), True),
    StructField("file_extension", StringType(), True)
])

def merge_target_file_metadata(target_files: List[Dict[str, str]]) -> bool:
    """
    Merge target files into the target_file_metadata Delta table using Delta Lake MERGE operation.
    
    Args:
        target_files: List of dictionaries containing file metadata with the following keys:
            - file_relative_path (str): Relative path of the file from the root directory
            - git_blob_hash (str): Git blob hash of the file content
            - category (str): Category of the file (e.g., 'research_findings')
            - file_name (str): Name of the file with extension
            - file_extension (str): Extension of the file without the dot
            Note: If present, 'file_path' key will be ignored
    
    Returns:
        bool: True if merge operation completed successfully. Raises exception on failure.
        
    Merge Conditions:
        1. Insert new record (file_status = 'pending_upload'):
           - When file_relative_path exists in target_files but not in table
        
        2. Update existing record (file_status = 'pending_update'):
           - When file_relative_path exists in both but git_blob_hash differs
        
        3. Mark as deleted (file_status = 'pending_delete'):
           - When file_relative_path exists in table but not in target_files
           
    Additional Info:
        - Uses standardized timestamp from common_utils for all datetime fields
        - Optional fields (blob_path, last_upload_at, error_message) default to NULL
        - Prints operation metrics after successful merge
    """
    try:
        # Get current timestamp
        timestamp_str = common_utils.get_standarized_timestamp()
        timestamp_val = to_timestamp(lit(timestamp_str))
        
        # Create DataFrame from target_files
        target_df = spark.createDataFrame(target_files, schema=TARGET_FILES_SCHEMA)
        
        # Get Delta table
        delta_table = DeltaTable.forName(spark, FILE_METADATA_TABLE)
        
        # Execute merge operation
        merge_operation = delta_table.alias("target").merge(
            source=target_df.alias("source"),
            condition="target.file_relative_path = source.file_relative_path"
        ).whenMatchedUpdate(
            condition="target.git_blob_hash != source.git_blob_hash",
            set={
                "git_blob_hash": "source.git_blob_hash",
                "file_status": lit("pending_update"),
                "etl_updated_at": timestamp_val
            }
        ).whenNotMatchedInsert(
            values={
                "file_relative_path": "source.file_relative_path",
                "git_blob_hash": "source.git_blob_hash",
                "file_status": lit("pending_upload"),
                "etl_created_at": timestamp_val,
                "etl_updated_at": timestamp_val,
                "category": "source.category",
                "file_name": "source.file_name",
                "file_extension": "source.file_extension"
            }
        ).whenNotMatchedBySourceUpdate(
            set={
                "file_status": lit("pending_delete"),
                "etl_updated_at": timestamp_val
            }
        )
        
        # Execute the merge
        merge_operation.execute()
        
        # Get operation metrics
        history = delta_table.history(1).collect()[0]
        num_inserted = history["operationMetrics"].get("numTargetRowsInserted", 0)
        num_updated = history["operationMetrics"].get("numTargetRowsUpdated", 0)
        num_deleted = history["operationMetrics"].get("numTargetRowsDeleted", 0)
        
        print(f"Merge operation completed:")
        print(f" - Files inserted (pending_upload): {num_inserted}")
        print(f" - Files updated (pending_update/delete): {num_updated}")
        print(f" - Files deleted: {num_deleted}")
        
        return True
        
    except Exception as e:
        print(f"Error during merge operation: {str(e)}")
        raise

# Example usage
def main():
    # Example target_files with actual data structure
    target_files = [
        {
            "file_path": "/tmp/va.gov-team-test/products/virtual-agent/research/controlled-study/research-report.md",
            "category": "research_findings",
            "file_relative_path": "products/virtual-agent/research/controlled-study/research-report.md",
            "git_blob_hash": "3ba7c774211b61de86ed4da242167449d85a62ec",
            "file_name": "research-report.md",
            "file_extension": "md"
        },
        {
            "file_path": "/tmp/va.gov-team-test/products/test/test_file.md",
            "category": "research_findings",
            "file_relative_path": "products/test/test_file.md",
            "git_blob_hash": "test-file-caeabfd1b42db32bd6a24836c490f99643c07ec2b",
            "file_name": "test_file.md",
            "file_extension": "md"
        }
    ]
    
    success = merge_target_file_metadata(target_files)
    if success:
        print("Merge completed successfully")
