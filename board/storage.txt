from pyspark.sql import SparkSession

def deduplicate_table(table_name):
    """
    Deduplicates records in the given table based on `git_blob_hash`.
    Keeps the record with the latest `etl_updated_at` and highest `id` in case of a tie.

    Args:
        table_name (str): The fully qualified table name (e.g., "vfs_raw.file_metadata").
    """
    # Initialize Spark session
    spark = SparkSession.builder.getOrCreate()

    # SQL query to deduplicate the table
    deduplication_query = f"""
        WITH RankedRecords AS (
            SELECT
                id,
                git_blob_hash,
                etl_updated_at,
                ROW_NUMBER() OVER (
                    PARTITION BY git_blob_hash
                    ORDER BY etl_updated_at DESC, id DESC
                ) AS rn
            FROM
                {table_name}
        )

        DELETE FROM
            {table_name}
        WHERE
            id IN (
                SELECT
                    id
                FROM
                    RankedRecords
                WHERE
                    rn > 1
            );
    """

    # Execute the deduplication query
    spark.sql(deduplication_query)
    print(f"Deduplication completed for table: {table_name}")

# Example usage
deduplicate_table("vfs_raw.file_metadata")
