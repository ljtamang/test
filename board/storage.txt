-- Create database if it doesn't exist
CREATE DATABASE IF NOT EXISTS vfs_raw;

-- Create the file_metadata table in the vfs_raw database with a custom location
CREATE TABLE IF NOT EXISTS vfs_raw.file_metadata (
    file_name STRING COMMENT 'Name of the file',
    file_relative_path STRING COMMENT 'Relative path of the file in the repository',
    file_extension STRING COMMENT 'File extension (e.g., md, txt, pdf)',
    category STRING COMMENT 'Category or type of the file (e.g., research_findings)',
    git_last_commit_date TIMESTAMP COMMENT 'Timestamp of the last Git commit for the file',
    git_blob_hash STRING COMMENT 'Git blob hash (unique identifier for the file content)',
    upload_status STRING COMMENT 'Status of the file upload (e.g., pending, needs_reupload, uploaded)',
    upload_on TIMESTAMP COMMENT 'Timestamp when the file was uploaded to Azure',
    blob_url STRING COMMENT 'URL of the file in Azure Blob Storage',
    error_message STRING COMMENT 'Error message if the file upload failed',
    etl_created_at TIMESTAMP COMMENT 'Timestamp when the record was created in the Delta table',
    etl_updated_at TIMESTAMP COMMENT 'Timestamp when the record was last updated in the Delta table'
)
LOCATION 'vfs/raw/metadata';


from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import to_timestamp, date_format, current_timestamp
from typing import List, Dict

def first_time_load_metadata_to_delta_table(metadata_list: List[Dict]):
    """
    Performs the first-time load of file metadata into the vfs_raw.file_metadata Delta table.
    Uses overwrite mode to ensure the table is fresh and doesn't contain duplicates.

    Args:
        metadata_list: List of dictionaries, where each dictionary contains metadata for a file.
                      Each dictionary must have keys: file_name, file_relative_path, file_extension,
                      category, git_last_commit_date, git_blob_hash, upload_status, upload_on, blob_url, error_message.
    """
    spark = SparkSession.builder.appName("FirstTimeLoadMetadata").getOrCreate()
    
    # Define the schema explicitly
    schema = StructType([
        StructField("file_name", StringType(), nullable=False),
        StructField("file_relative_path", StringType(), nullable=False),
        StructField("file_extension", StringType(), nullable=False),
        StructField("category", StringType(), nullable=False),
        StructField("git_last_commit_date", StringType(), nullable=False),  # Temporarily as StringType
        StructField("git_blob_hash", StringType(), nullable=False),
        StructField("upload_status", StringType(), nullable=False),
        StructField("upload_on", StringType(), nullable=True),  # Temporarily as StringType
        StructField("blob_url", StringType(), nullable=True),
        StructField("error_message", StringType(), nullable=True)
    ])
    
    # Convert metadata list to DataFrame with explicit schema
    initial_df = spark.createDataFrame(metadata_list, schema=schema)
    
    # Convert git_last_commit_date from string to TimestampType
    # Assuming the format is ISO 8601 with timezone (e.g., "2024-11-25T11:30:31-05:00")
    initial_df = initial_df.withColumn(
        "git_last_commit_date",
        to_timestamp("git_last_commit_date", "yyyy-MM-dd'T'HH:mm:ssXXX")  # ISO 8601 format with timezone
    )
    
    # Convert upload_on from string to TimestampType (if provided)
    initial_df = initial_df.withColumn(
        "upload_on",
        to_timestamp("upload_on", "yyyy-MM-dd'T'HH:mm:ssXXX")  # ISO 8601 format with timezone
    )
    
    # Add etl_created_at and etl_updated_at fields with current timestamp in ISO 8601 format
    initial_df = initial_df.withColumn("etl_created_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ")) \
                           .withColumn("etl_updated_at", date_format(current_timestamp(), "yyyy-MM-dd'T'HH:mm:ss.SSSZ"))
    
    # Insert all records into the Delta table in overwrite mode
    initial_df.write.format("delta") \
        .mode("overwrite") \  # Overwrite the table to ensure it's fresh
        .saveAsTable("vfs_raw.file_metadata")
    
    print("First-time load of metadata into Delta table completed successfully.")
