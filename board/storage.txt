from pyspark.sql import SparkSession
from typing import Dict

def remove_duplicates_by_hash(table_name: str) -> Dict[str, int]:
   """
   Removes duplicate records based on git_blob_hash, keeping one record when duplicates exist.
   """
   spark = SparkSession.builder.getOrCreate()
   
   initial_count = spark.table(table_name).count()
   
   spark.sql(f"""
       DELETE FROM {table_name} 
       WHERE git_blob_hash IN (
           SELECT git_blob_hash
           FROM {table_name}
           GROUP BY git_blob_hash
           HAVING count(*) > 1
       )
       AND _id NOT IN (
           SELECT MIN(_id)
           FROM {table_name}
           GROUP BY git_blob_hash
       )
   """)
   
   final_count = spark.table(table_name).count()
   
   return {
       "total_records": initial_count,
       "deleted_records": initial_count - final_count
   }
