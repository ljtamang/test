from pathlib import Path
from typing import Dict, Any, List, Optional
import re
import os
from collections import defaultdict

# Configuration
MIN_CONTENT_LENGTH = 20  # Minimum words to consider
MIN_PLAN_SCORE = 5
MIN_FINDINGS_SCORE = 6
MIN_GUIDE_SCORE = 5

# Caching
content_cache = {}
stats = defaultdict(int)

def normalize_filename(filename: str) -> str:
    """Normalize filename for consistent checking."""
    filename = Path(filename).stem.lower()
    filename = re.sub(r'[^a-z0-9]', '_', filename)
    filename = re.sub(r'_+', '_', filename)
    return filename.strip('_')

def read_file_content(file_path: str) -> Optional[str]:
    """Reads content of a local file with caching."""
    if file_path in content_cache:
        return content_cache[file_path]
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            content_cache[file_path] = content
            return content
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return None

def is_valid_content(content: str) -> bool:
    """Check if content meets minimum requirements for processing."""
    return content and len(content.split()) >= MIN_CONTENT_LENGTH

def tag_as_research_plan(file_path: str, min_threshold: int = MIN_PLAN_SCORE) -> Optional[Dict[str, Any]]:
    """Tag files as research plans with enhanced scoring."""
    penalty_keywords = {'guide', 'findings', 'report', 'summary'}
    priority_keywords = [
        'research-plan', 'research_plan', 'researchplan', 'research plan', 
        'uat-plan', 'research_plan_', 'research-plan-', 'researchplan_'
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize findings/guide content
        if any(kw in content_lower for kw in ['# findings', '# conversation guide']):
            score -= 5
        
        # Reward plan content
        plan_headers = [
            "## background", "## research goals", "## methodology", 
            "## recruitment", "## timeline", "## team roles"
        ]
        for header in plan_headers:
            if header in content_lower:
                score += 2
                break
        
        if "research plan for" in content_lower:
            score += 5
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and "plan" in path_str:
        score += 3
    
    if score >= min_threshold:
        return {'tag': 'research_plan', 'score': score}
    return None

def tag_as_research_findings(file_path: str, min_threshold: int = MIN_FINDINGS_SCORE) -> Optional[Dict[str, Any]]:
    """Tag files as research findings with enhanced scoring."""
    penalty_keywords = {'plan', 'guide', 'readme'}
    priority_keywords = [
        'research-findings', 'research_findings', 'researchfindings',
        'research findings', 'findings', 'result', 'results'
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize plan/guide content
        if any(kw in content_lower for kw in ['# research plan', '# conversation guide']):
            score -= 5
        
        # Reward findings content
        findings_headers = [
            "# research findings", "## key findings",
            "# findings", "## recommendations"
        ]
        for header in findings_headers:
            if header in content_lower:
                score += 2
                break
        
        if "key findings" in content_lower and "recommendations" in content_lower:
            score += 6
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and "findings" in path_str:
        score += 3
    
    if score >= min_threshold:
        return {'tag': 'research_findings', 'score': score}
    return None

def tag_as_guide(file_path: str, min_threshold: int = MIN_GUIDE_SCORE) -> Optional[Dict[str, Any]]:
    """Tag files as conversation guides with enhanced scoring."""
    penalty_keywords = {'plan', 'findings', 'report'}
    priority_keywords = [
        'conversation-guide', 'convo-guide', 'conversation_guide',
        'convo_guide', 'interview-guide', 'discussion-guide'
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize plan/findings content
        if any(kw in content_lower for kw in ['# research plan', '# findings']):
            score -= 5
        
        # Reward guide content
        guide_headers = [
            "# conversation guide", "## warm-up questions",
            "## task completion", "## thank you and closing"
        ]
        for header in guide_headers:
            if header in content_lower:
                score += 2
                break
        
        if "conversation guide" in content_lower:
            score += 7
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and ("interview" in path_str or "usability" in path_str):
        score += 1
    
    if score >= min_threshold:
        return {'tag': 'conversation_guide', 'score': score}
    return None

def keyword_based_tagging(file_path: str) -> Dict[str, Any]:
    """Tags files by sequentially applying different tagging strategies."""
    content = read_file_content(file_path)
    if not content or not is_valid_content(content):
        return {
            'tag': 'invalid_content',
            'score': 0
        }
    
    # First try to tag as research findings
    tag = tag_as_research_findings(file_path)
    if tag:
        return tag
    
    # If not a research finding, try to tag as research plan
    tag = tag_as_research_plan(file_path)
    if tag:
        return tag
    
    # If not a research plan, try to tag as conversation guide
    tag = tag_as_guide(file_path)
    if tag:
        return tag
    
    # If neither, mark as unclassified
    return {
        'tag': 'unclassified',
        'score': 1
    }

def tag_files(file_paths: List[str]) -> List[Dict[str, Any]]:
    """Main function to process and tag multiple files."""
    tagged_files = []
    
    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File not found - {file_path}")
            continue
        
        file_tags = {
            'file_path': file_path,
            'tags': [keyword_based_tagging(file_path)]  # Wrap in list to maintain consistent structure
        }
        tagged_files.append(file_tags)
    
    return tagged_files

def test_accuracy(test_files_dir: str) -> Dict[str, Any]:
    """
    Test classification accuracy by reading test file paths from text files.
    Assumes files are locally available and uses tag_files() for classification.
    
    Args:
        test_files_dir: Directory containing the test files (plan_path.txt, findings_path.txt, guide_path.txt)
        
    Returns:
        Dictionary containing accuracy results for each category
    """
    # Read test file paths
    def read_test_paths(filename):
        try:
            with open(os.path.join(test_files_dir, filename), 'r') as f:
                return [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Warning: Test file {filename} not found")
            return []
    
    # Get paths for each category
    plan_paths = read_test_paths("plan_path.txt")
    findings_paths = read_test_paths("findings_path.txt")
    guide_paths = read_test_paths("guide_path.txt")
    
    # Tag all files and organize by expected category
    all_files = []
    all_files.extend([(path, 'research_plan') for path in plan_paths])
    all_files.extend([(path, 'research_findings') for path in findings_paths])
    all_files.extend([(path, 'conversation_guide') for path in guide_paths])
    
    # Get just the paths for tagging
    file_paths = [file[0] for file in all_files]
    
    # Tag all files
    tagged_files = tag_files(file_paths)
    
    # Create a mapping of file path to predicted tag
    pred_tags = {}
    for file in tagged_files:
        path = file['file_path']
        # Get the first tag (since tag_files returns a list with one element)
        pred_tags[path] = file['tags'][0]['tag'] if file['tags'] else 'unclassified'
    
    # Calculate accuracy for each category
    def calculate_metrics(expected_files, expected_tag):
        correct = 0
        incorrect = 0
        not_found = 0
        details = []
        
        for path, expected in expected_files:
            if path not in pred_tags:
                not_found += 1
                details.append({
                    'file': path,
                    'expected': expected,
                    'predicted': 'file_not_found',
                    'correct': False
                })
                continue
            
            predicted = pred_tags[path]
            is_correct = predicted == expected
            if is_correct:
                correct += 1
            else:
                incorrect += 1
            
            details.append({
                'file': path,
                'expected': expected,
                'predicted': predicted,
                'correct': is_correct
            })
        
        total = correct + incorrect
        accuracy = (correct / total * 100) if total > 0 else 0
        
        return {
            'total_files': len(expected_files),
            'files_found': total,
            'files_not_found': not_found,
            'correct': correct,
            'incorrect': incorrect,
            'accuracy': accuracy,
            'details': details
        }
    
    # Calculate for each category
    plan_results = calculate_metrics([(p, 'research_plan') for p in plan_paths], 'research_plan')
    findings_results = calculate_metrics([(p, 'research_findings') for p in findings_paths], 'research_findings')
    guide_results = calculate_metrics([(p, 'conversation_guide') for p in guide_paths], 'conversation_guide')
    
    # Calculate overall accuracy
    total_correct = plan_results['correct'] + findings_results['correct'] + guide_results['correct']
    total_files = plan_results['files_found'] + findings_results['files_found'] + guide_results['files_found']
    overall_accuracy = (total_correct / total_files * 100) if total_files > 0 else 0
    
    return {
        'research_plans': plan_results,
        'research_findings': findings_results,
        'conversation_guides': guide_results,
        'overall_accuracy': overall_accuracy,
        'total_files_processed': total_files,
        'total_correct': total_correct,
        'total_incorrect': total_files - total_correct
    }

def print_results(results: Dict[str, Any]):
    """Print the test results in a readable format."""
    print("\n=== Classification Accuracy Results ===")
    print(f"\nOverall Accuracy: {results['overall_accuracy']:.2f}%")
    print(f"Total Files Processed: {results['total_files_processed']}")
    print(f"Total Correct: {results['total_correct']}")
    print(f"Total Incorrect: {results['total_incorrect']}")
    
    # Print detailed results for each category
    for category in ['research_plans', 'research_findings', 'conversation_guides']:
        data = results[category]
        print(f"\n{category.replace('_', ' ').title()}:")
        print(f"  Accuracy: {data['accuracy']:.2f}%")
        print(f"  Correct: {data['correct']}")
        print(f"  Incorrect: {data['incorrect']}")
        print(f"  Files Not Found: {data['files_not_found']}")
        print(f"  Total Files: {data['total_files']}")

if __name__ == "__main__":
    # Example usage - assumes the text files are in the current directory
    print("Running document classification accuracy test...")
    results = test_accuracy('.')
    print_results(results)
