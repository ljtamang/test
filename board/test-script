def test_accuracy(test_files_dir: str) -> Dict[str, Any]:
    """
    Test classification accuracy by reading test file paths from text files and checking local files.
    Assumes files are locally available and uses tag_files() for classification.
    
    Args:
        test_files_dir: Directory containing the test files (plan_path.txt, findings_path.txt, guide_path.txt)
        
    Returns:
        Dictionary containing accuracy results for each category
    """
    # Read test file paths
    def read_test_paths(filename):
        try:
            with open(os.path.join(test_files_dir, filename), 'r') as f:
                return [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Warning: Test file {filename} not found")
            return []
    
    # Get paths for each category
    plan_paths = read_test_paths("plan_path.txt")
    findings_paths = read_test_paths("findings_path.txt")
    guide_paths = read_test_paths("guide_path.txt")
    
    # Tag all files and organize by expected category
    all_files = []
    all_files.extend([(path, 'research_plan') for path in plan_paths])
    all_files.extend([(path, 'research_findings') for path in findings_paths])
    all_files.extend([(path, 'conversation_guide') for path in guide_paths])
    
    # Get just the paths for tagging
    file_paths = [file[0] for file in all_files]
    
    # Tag all files
    tagged_files = tag_files(file_paths)
    
    # Create a mapping of file path to predicted tag
    pred_tags = {}
    for file in tagged_files:
        path = file['file_path']
        # Get the first tag (since tag_files returns a list with one element)
        pred_tags[path] = file['tags'][0]['tag'] if file['tags'] else 'unclassified'
    
    # Calculate accuracy for each category
    def calculate_metrics(expected_files, expected_tag):
        correct = 0
        incorrect = 0
        not_found = 0
        details = []
        
        for path, expected in expected_files:
            if path not in pred_tags:
                not_found += 1
                details.append({
                    'file': path,
                    'expected': expected,
                    'predicted': 'file_not_found',
                    'correct': False
                })
                continue
            
            predicted = pred_tags[path]
            is_correct = predicted == expected
            if is_correct:
                correct += 1
            else:
                incorrect += 1
            
            details.append({
                'file': path,
                'expected': expected,
                'predicted': predicted,
                'correct': is_correct
            })
        
        total = correct + incorrect
        accuracy = (correct / total * 100) if total > 0 else 0
        
        return {
            'total_files': len(expected_files),
            'files_found': total,
            'files_not_found': not_found,
            'correct': correct,
            'incorrect': incorrect,
            'accuracy': accuracy,
            'details': details
        }
    
    # Calculate for each category
    plan_results = calculate_metrics([(p, 'research_plan') for p in plan_paths], 'research_plan')
    findings_results = calculate_metrics([(p, 'research_findings') for p in findings_paths], 'research_findings')
    guide_results = calculate_metrics([(p, 'conversation_guide') for p in guide_paths], 'conversation_guide')
    
    # Calculate overall accuracy
    total_correct = plan_results['correct'] + findings_results['correct'] + guide_results['correct']
    total_files = plan_results['files_found'] + findings_results['files_found'] + guide_results['files_found']
    overall_accuracy = (total_correct / total_files * 100) if total_files > 0 else 0
    
    return {
        'research_plans': plan_results,
        'research_findings': findings_results,
        'conversation_guides': guide_results,
        'overall_accuracy': overall_accuracy,
        'total_files_processed': total_files,
        'total_correct': total_correct,
        'total_incorrect': total_files - total_correct
    }
