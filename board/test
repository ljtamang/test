# Project Structure:
project_root/
├── path_setup.py
├── utils/
│   ├── __init__.py
│   ├── file_utils.py
│   └── git_utils.py
├── services/
│   ├── __init__.py
│   ├── file_tagger.py
│   └── metadata_service.py
├── configs/
│   └── bronze_config.json
├── bronze_etl.ipynb
└── silver_etl.ipynb


# path_setup.py
import os
import sys
from pathlib import Path

def setup_project_path():
    """
    Add project root to Python path if not already present.
    Handles both Databricks and local execution contexts.
    """
    try:
        # Try Databricks context
        from pyspark.dbutils import DBUtils
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.getOrCreate()
        dbutils = DBUtils(spark)
        
        # Get notebook path
        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getPath().get()
        project_root = Path(notebook_path).parent
        
    except Exception as e:
        # Fallback to current directory if not in Databricks
        project_root = Path(os.getcwd())
    
    project_root_str = str(project_root)
    
    if project_root_str not in sys.path:
        sys.path.insert(0, project_root_str)
        print(f"Added to Python path: {project_root_str}")
    
    return project_root

# Run setup when this module is imported
project_root = setup_project_path()

if __name__ == "__main__":
    print("\nProject root:", project_root)
    print("\nPython path:")
    for path in sys.path:
        print(f"- {path}")
