

 Add project root to path if running directly
if __name__ == '__main__':
    # Get the project root (two directories up from this script)
    current_file = Path(__file__).resolve()
    project_root = str(current_file.parent.parent)
    
    # Add project root to path if not already there
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    
    # Now import path_setup
    from path_setup import setup_script_paths
    PROJECT_ROOT = setup_script_paths()

# Import required modules (this will work both when running as main or when imported)
try:
    from utils.file_utils import read_file, write_file
    from utils.git_utils import get_git_info
except ImportError as e:
    # If imports fail, try setting up paths and importing again
    from path_setup import setup_script_paths
    setup_script_paths()
    from utils.file_utils import read_file, write_file
    from utils.git_utils import get_git_info


=============
# path_setup.py
import os
import sys
from pathlib import Path

def setup_project_paths():
    """
    Sets up the project paths for Databricks environment
    Returns the project root path
    """
    # In Databricks, you might want to use a mounted path or workspace path
    try:
        # If running in Databricks
        project_root = "/Workspace/Repos/your_username/your_project"  # Adjust this path
    except:
        # If running locally
        project_root = Path(__file__).parent.absolute()
    
    # Add project paths to Python path
    paths_to_add = [
        project_root,
        os.path.join(project_root, 'utils'),
        os.path.join(project_root, 'services')
    ]
    
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.append(str(path))
    
    return project_root

# Execute path setup when imported
PROJECT_ROOT = setup_project_paths()

# ==============================================================================
# utils/__init__.py
from utils.file_utils import *
from utils.git_utils import *

__all__ = [
    'read_file',
    'write_file',
    'get_git_info',
    'get_latest_commit'
]

# ==============================================================================
# services/__init__.py
from services.file_tagger import *
from services.metadata_service import *

__all__ = [
    'tag_file',
    'get_tags',
    'extract_metadata',
    'update_metadata'
]

# ==============================================================================
# services/file_tagger.py
import os
from pathlib import Path
from typing import List, Dict
from pyspark.sql import SparkSession

# Import from utils package
from utils.file_utils import read_file, write_file
from utils.git_utils import get_git_info

# Import from services package
from services.metadata_service import extract_metadata, update_metadata

def tag_file(spark: SparkSession, file_path: str) -> Dict:
    """Example function implementation"""
    # Your implementation here
    pass

def get_tags(spark: SparkSession, file_path: str) -> List[str]:
    """Example function implementation"""
    # Your implementation here
    pass

# ==============================================================================
# bronze_etl.ipynb
# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze ETL Notebook

# COMMAND ----------
# MAGIC %pip install your_required_packages

# COMMAND ----------
import sys
from path_setup import setup_project_paths

# Setup paths
PROJECT_ROOT = setup_project_paths()

# Create Spark session (already available in Databricks as 'spark')
# from pyspark.sql import SparkSession
# spark = SparkSession.builder.appName("bronze_etl").getOrCreate()

# Import necessary modules
from utils.file_utils import read_file
from utils.git_utils import get_git_info
from services.file_tagger import tag_file
from services.metadata_service import extract_metadata

# Your ETL code here
# Example:
# df = spark.read.format("delta").load("/path/to/bronze/table")

# ==============================================================================
# silver_etl.ipynb
# Databricks notebook source
# MAGIC %md
# MAGIC # Silver ETL Notebook

# COMMAND ----------
# MAGIC %pip install your_required_packages

# COMMAND ----------
import sys
from path_setup import setup_project_paths

# Setup paths
PROJECT_ROOT = setup_project_paths()

# Import entire modules
from utils import file_utils, git_utils
from services import file_tagger, metadata_service

# Or import specific functions
from utils.file_utils import read_file, write_file
from services.metadata_service import update_metadata

# Your ETL code here
# Example:
# bronze_df = spark.read.format("delta").load("/path/to/bronze/table")
# silver_df = transform_to_silver(bronze_df)
# silver_df.write.format("delta").mode("overwrite").save("/path/to/silver/table")

# ==============================================================================
# Additional Databricks-specific considerations:

# 1. To use this in Databricks:
#    - Upload these files to your Databricks workspace or use Databricks Repos
#    - Update the project_root path in path_setup.py to match your workspace structure
#    - Install any required packages using %pip magic command

# 2. For production:
#    - Consider using Databricks Workflows for orchestration
#    - Use Databricks Secrets for sensitive information
#    - Implement proper error handling and logging

# 3. Best practices:
#    - Use Delta Lake for all table operations
#    - Implement proper schema evolution strategies
#    - Use DBUtils for file operations when possible
#    - Leverage Databricks widgets for parameters
