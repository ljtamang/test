from pathlib import Path
from typing import Dict, Any, List, Optional
import re
import os
from collections import defaultdict

# Configuration constants
MIN_CONTENT_LENGTH = 20  # Minimum words to consider for valid content
MIN_PLAN_SCORE = 5       # Minimum score to classify as research plan
MIN_FINDINGS_SCORE = 6   # Minimum score to classify as research findings
MIN_GUIDE_SCORE = 5      # Minimum score to classify as conversation guide

# Caching dictionaries
content_cache = {}       # Cache for file contents to avoid repeated reads
stats = defaultdict(int) # Statistics counter for various operations

def normalize_filename(filename: str) -> str:
    """
    Normalize filename to a consistent format for comparison.
    
    Converts to lowercase, removes hyphens, replaces underscores with spaces,
    and normalizes whitespace.
    
    Args:
        filename: Input filename to be normalized
        
    Returns:
        Normalized filename string
    """
    normalized = filename.lower()
    normalized = normalized.replace('-', '').replace('_', ' ')
    normalized = ' '.join(normalized.split())
    return normalized

def read_file_content(file_path: str) -> Optional[str]:
    """
    Read content from a local file with caching.
    
    Args:
        file_path: Path to the file to read
        
    Returns:
        Content of the file as string if successful, None if reading fails
    """
    if file_path in content_cache:
        return content_cache[file_path]
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            content_cache[file_path] = content
            return content
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return None

def is_valid_content(content: str) -> bool:
    """
    Check if content meets minimum requirements for processing.
    
    Args:
        content: Text content to validate
        
    Returns:
        True if content is valid (non-empty and meets minimum length), False otherwise
    """
    return content and len(content.split()) >= MIN_CONTENT_LENGTH

def tag_as_research_plan(file_path: str, min_threshold: int = MIN_PLAN_SCORE) -> Optional[Dict[str, Any]]:
    """
    Determine if a file should be classified as a research plan document.
    
    Uses filename patterns, content analysis, and path analysis to calculate
    a classification score. Returns classification if score meets threshold.
    
    Args:
        file_path: Path to the file to evaluate
        min_threshold: Minimum score required for classification (default: MIN_PLAN_SCORE)
        
    Returns:
        Dictionary with 'tag' and 'score' if classified as research plan,
        None if score is below threshold
    """
    penalty_keywords = {'guide', 'findings', 'report', 'summary'}
    priority_keywords = [
        'research plan', 'researchplan', 'research plan', 
        'uat plan', 'research plan ', 'research plan ', 'researchplan '
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize findings/guide content
        if any(kw in content_lower for kw in ['# findings', '# conversation guide']):
            score -= 5
        
        # Reward plan content
        plan_headers = [
            "## background", "## research goals", "## methodology", 
            "## recruitment", "## timeline", "## team roles"
        ]
        for header in plan_headers:
            if header in content_lower:
                score += 2
                break
        
        if "research plan for" in content_lower:
            score += 5
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and "plan" in path_str:
        score += 3
    
    if score >= min_threshold:
        return {'tag': 'research_plan', 'score': score}
    return None

def tag_as_research_findings(file_path: str, min_threshold: int = MIN_FINDINGS_SCORE) -> Optional[Dict[str, Any]]:
    """
    Determine if a file should be classified as a research findings document.
    
    Uses filename patterns, content analysis, and path analysis to calculate
    a classification score. Returns classification if score meets threshold.
    
    Args:
        file_path: Path to the file to evaluate
        min_threshold: Minimum score required for classification (default: MIN_FINDINGS_SCORE)
        
    Returns:
        Dictionary with 'tag' and 'score' if classified as research findings,
        None if score is below threshold
    """
    penalty_keywords = {'plan', 'guide', 'readme'}
    priority_keywords = [
        'research findings', 'researchfindings',
        'research findings', 'findings', 'result', 'results'
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize plan/guide content
        if any(kw in content_lower for kw in ['# research plan', '# conversation guide']):
            score -= 5
        
        # Reward findings content
        findings_headers = [
            "# research findings", "## key findings",
            "# findings", "## recommendations"
        ]
        for header in findings_headers:
            if header in content_lower:
                score += 2
                break
        
        if "key findings" in content_lower and "recommendations" in content_lower:
            score += 6
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and "findings" in path_str:
        score += 3
    
    if score >= min_threshold:
        return {'tag': 'research_findings', 'score': score}
    return None

def tag_as_guide(file_path: str, min_threshold: int = MIN_GUIDE_SCORE) -> Optional[Dict[str, Any]]:
    """
    Determine if a file should be classified as a conversation guide document.
    
    Uses filename patterns, content analysis, and path analysis to calculate
    a classification score. Returns classification if score meets threshold.
    
    Args:
        file_path: Path to the file to evaluate
        min_threshold: Minimum score required for classification (default: MIN_GUIDE_SCORE)
        
    Returns:
        Dictionary with 'tag' and 'score' if classified as conversation guide,
        None if score is below threshold
    """
    penalty_keywords = {'plan', 'findings', 'report'}
    priority_keywords = [
        'conversation guide', 'convo guide', 'conversation guide',
        'convo guide', 'interview guide', 'discussion guide'
    ]
    
    filename = normalize_filename(Path(file_path).name)
    score = 0
    
    # Penalty check
    for kw in penalty_keywords:
        if kw in filename:
            score -= 8
            break
    
    # Priority filename patterns
    if any(p in filename for p in priority_keywords):
        score += 10
    
    # Content analysis
    content = read_file_content(file_path)
    if content and is_valid_content(content):
        content_lower = content.lower()
        
        # Penalize plan/findings content
        if any(kw in content_lower for kw in ['# research plan', '# findings']):
            score -= 5
        
        # Reward guide content
        guide_headers = [
            "# conversation guide", "## warm-up questions",
            "## task completion", "## thank you and closing"
        ]
        for header in guide_headers:
            if header in content_lower:
                score += 2
                break
        
        if "conversation guide" in content_lower:
            score += 7
    
    # Path analysis
    path_str = normalize_filename(str(Path(file_path).parent))
    if "research" in path_str and ("interview" in path_str or "usability" in path_str):
        score += 1
    
    if score >= min_threshold:
        return {'tag': 'conversation_guide', 'score': score}
    return None

def keyword_based_tagging(file_path: str) -> Dict[str, Any]:
    """
    Classify a file by sequentially applying specialized taggers.
    
    Tries to classify as research findings first, then research plan,
    then conversation guide. Falls back to 'unclassified' if none match.
    
    Args:
        file_path: Path to the file to classify
        
    Returns:
        Dictionary containing:
        - 'tag': classification result
        - 'score': confidence score
    """
    content = read_file_content(file_path)
    if not content or not is_valid_content(content):
        return {
            'tag': 'invalid_content',
            'score': 0
        }
    
    # Try classification in order of priority
    tag = tag_as_research_findings(file_path)
    if tag:
        return tag
    
    tag = tag_as_research_plan(file_path)
    if tag:
        return tag
    
    tag = tag_as_guide(file_path)
    if tag:
        return tag
    
    # Fallback classification
    return {
        'tag': 'unclassified',
        'score': 1
    }

def tag_files(file_paths: List[str]) -> List[Dict[str, Any]]:
    """
    Classify multiple files and return their tags.
    
    Args:
        file_paths: List of file paths to classify
        
    Returns:
        List of dictionaries, each containing:
        - 'file_path': original file path
        - 'tags': list of classification results (usually one element)
    """
    tagged_files = []
    
    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File not found - {file_path}")
            continue
        
        file_tags = {
            'file_path': file_path,
            'tags': [keyword_based_tagging(file_path)]
        }
        tagged_files.append(file_tags)
    
    return tagged_files

def test_accuracy(test_files_dir: str) -> Dict[str, Any]:
    """
    Test classification accuracy against known test cases.
    
    Reads test files from plan_path.txt, findings_path.txt, and guide_path.txt,
    then evaluates classifier performance against these known categories.
    
    Args:
        test_files_dir: Directory containing the test files
        
    Returns:
        Dictionary containing comprehensive accuracy results:
        - 'research_plans': metrics for research plan classification
        - 'research_findings': metrics for findings classification
        - 'conversation_guides': metrics for guide classification
        - 'overall_accuracy': combined accuracy across all categories
        - 'total_files_processed': total files successfully classified
        - 'total_correct': total correct classifications
        - 'total_incorrect': total incorrect classifications
    """
    def read_test_paths(filename):
        """Helper to read test file paths from a text file."""
        try:
            with open(os.path.join(test_files_dir, filename), 'r') as f:
                return [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Warning: Test file {filename} not found")
            return []
    
    # Load test file paths
    plan_paths = read_test_paths("plan_path.txt")
    findings_paths = read_test_paths("findings_path.txt")
    guide_paths = read_test_paths("guide_path.txt")
    
    # Prepare all files with their expected categories
    all_files = []
    all_files.extend([(path, 'research_plan') for path in plan_paths])
    all_files.extend([(path, 'research_findings') for path in findings_paths])
    all_files.extend([(path, 'conversation_guide') for path in guide_paths])
    
    # Classify all files
    file_paths = [file[0] for file in all_files]
    tagged_files = tag_files(file_paths)
    
    # Create prediction mapping
    pred_tags = {}
    for file in tagged_files:
        path = file['file_path']
        pred_tags[path] = file['tags'][0]['tag'] if file['tags'] else 'unclassified'
    
    def calculate_metrics(expected_files, expected_tag):
        """
        Calculate classification metrics for a specific document type.
        
        Args:
            expected_files: List of (path, expected_tag) tuples
            expected_tag: The correct tag for these files
            
        Returns:
            Dictionary of metrics including accuracy, counts, and details
        """
        correct = 0
        incorrect = 0
        not_found = 0
        details = []
        
        for path, expected in expected_files:
            if path not in pred_tags:
                not_found += 1
                details.append({
                    'file': path,
                    'expected': expected,
                    'predicted': 'file_not_found',
                    'correct': False
                })
                continue
            
            predicted = pred_tags[path]
            is_correct = predicted == expected
            if is_correct:
                correct += 1
            else:
                incorrect += 1
            
            details.append({
                'file': path,
                'expected': expected,
                'predicted': predicted,
                'correct': is_correct
            })
        
        total = correct + incorrect
        accuracy = (correct / total * 100) if total > 0 else 0
        
        return {
            'total_files': len(expected_files),
            'files_found': total,
            'files_not_found': not_found,
            'correct': correct,
            'incorrect': incorrect,
            'accuracy': accuracy,
            'details': details
        }
    
    # Calculate metrics for each category
    plan_results = calculate_metrics([(p, 'research_plan') for p in plan_paths], 'research_plan')
    findings_results = calculate_metrics([(p, 'research_findings') for p in findings_paths], 'research_findings')
    guide_results = calculate_metrics([(p, 'conversation_guide') for p in guide_paths], 'conversation_guide')
    
    # Calculate overall statistics
    total_correct = plan_results['correct'] + findings_results['correct'] + guide_results['correct']
    total_files = plan_results['files_found'] + findings_results['files_found'] + guide_results['files_found']
    overall_accuracy = (total_correct / total_files * 100) if total_files > 0 else 0
    
    return {
        'research_plans': plan_results,
        'research_findings': findings_results,
        'conversation_guides': guide_results,
        'overall_accuracy': overall_accuracy,
        'total_files_processed': total_files,
        'total_correct': total_correct,
        'total_incorrect': total_files - total_correct
    }

def print_results(results: Dict[str, Any]):
    """
    Print classification accuracy results in human-readable format.
    
    Args:
        results: Dictionary of results from test_accuracy()
    """
    print("\n=== Classification Accuracy Results ===")
    print(f"\nOverall Accuracy: {results['overall_accuracy']:.2f}%")
    print(f"Total Files Processed: {results['total_files_processed']}")
    print(f"Total Correct: {results['total_correct']}")
    print(f"Total Incorrect: {results['total_incorrect']}")
    
    for category in ['research_plans', 'research_findings', 'conversation_guides']:
        data = results[category]
        print(f"\n{category.replace('_', ' ').title()}:")
        print(f"  Accuracy: {data['accuracy']:.2f}%")
        print(f"  Correct: {data['correct']}")
        print(f"  Incorrect: {data['incorrect']}")
        print(f"  Files Not Found: {data['files_not_found']}")
        print(f"  Total Files: {data['total_files']}")

if __name__ == "__main__":
    print("Running document classification accuracy test...")
    results = test_accuracy('.')
    print_results(results)
