Complete Git Metadata Extractor with Improved Error Handling

import os
import git
from typing import Dict, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

class GitMetadataExtractor:
    def __init__(self, repo_path: str, file_types: Optional[List[str]] = None):
        """
        Initialize extractor with Git repository path and optional file type filter.
        
        Parameters:
        repo_path (str): Path to Git repository root
        file_types (List[str], optional): List of file extensions to process (without dots)
        """
        self.repo = git.Repo(repo_path)
        self.repo_root = repo_path
        self.file_types = {ext.lstrip('.').lower() for ext in file_types} if file_types and len(file_types) > 0 else None

    def calculate_optimal_workers(self) -> int:
        """Calculate optimal number of worker threads based on environment."""
        try:
            from pyspark import SparkContext
            sc = SparkContext.getOrCreate()
            max_workers = sc.defaultParallelism
        except:
            max_workers = 32
        
        cpu_count = os.cpu_count() or 1
        suggested_workers = cpu_count * 4
        return min(suggested_workers, max_workers)

    def is_valid_file_type(self, file_name: str) -> bool:
        """Check if file type should be processed based on filter."""
        if self.file_types is None:
            return True
        file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
        return file_ext in self.file_types

    def extract_single_file_metadata(self, file_path: str) -> Optional[Dict]:
        """Extract metadata for a single file within the Git repository."""
        try:
            file_name = os.path.basename(file_path)
            
            if not self.is_valid_file_type(file_name):
                return None

            file_stats = os.stat(file_path)
            file_type = os.path.splitext(file_name)[1].lower().lstrip('.')
            rel_path = os.path.relpath(file_path, self.repo_root)
            
            file_info = {
                'file_name': file_name,
                'file_path': os.path.abspath(file_path),
                'file_size': format_file_size(file_stats.st_size),
                'file_type': file_type,
                'relative_path': rel_path,
                'latest_commit_date': None
            }
            
            try:
                # Try to get commit history with more robust error handling
                commits = []
                try:
                    # First try with regular iterator
                    commits = list(self.repo.iter_commits(paths=rel_path))
                except ValueError as ve:
                    if "SHA b'tree'" in str(ve):
                        # Try alternative method using git log
                        commits = list(self.repo.iter_commits(paths=rel_path, max_count=1))
                except Exception as e:
                    # Log error but continue processing
                    pass

                if commits:
                    file_info['latest_commit_date'] = commits[0].committed_datetime.isoformat()
                
                return file_info
                
            except Exception as git_error:
                # If git operations fail, still return the file info without commit date
                return file_info
                
        except Exception as e:
            return None
    def _batch_get_commit_dates(self, rel_paths: List[str]) -> Dict[str, str]:
    """
    Get commit dates for multiple files in batch with multiple fallback methods.
    """
    commit_dates = {}
    
    # Method 1: Using iter_commits and stats (fastest)
    try:
        for commit in self.repo.iter_commits():
            for rel_path in rel_paths:
                if rel_path not in commit_dates:
                    try:
                        if rel_path in commit.stats.files:
                            commit_dates[rel_path] = commit.committed_datetime.isoformat()
                    except:
                        continue
    except:
        pass

    # Method 2: Try git log for files without commit dates
    remaining_paths = [path for path in rel_paths if path not in commit_dates]
    if remaining_paths:
        for rel_path in remaining_paths:
            try:
                commits = list(self.repo.iter_commits(paths=rel_path, max_count=1))
                if commits:
                    commit_dates[rel_path] = commits[0].committed_datetime.isoformat()
            except:
                pass

    # Method 3: Use git blame as last resort
    remaining_paths = [path for path in rel_paths if path not in commit_dates]
    if remaining_paths:
        for rel_path in remaining_paths:
            try:
                full_path = os.path.join(self.repo_root, rel_path)
                if os.path.exists(full_path):
                    blame = self.repo.blame('HEAD', rel_path)
                    if blame and blame[0]:
                        commit = blame[0][0]
                        commit_dates[rel_path] = commit.committed_datetime.isoformat()
            except:
                pass

    return commit_dates
    def process_files_batch(self, file_paths: Optional[List[str]] = None) -> List[Dict]:
        """Process multiple files in parallel using ThreadPoolExecutor."""
        if file_paths is None:
            file_paths = []
            for root, _, files in os.walk(self.repo_root):
                if '.git' in root:
                    continue
                for file in files:
                    file_paths.append(os.path.join(root, file))

        num_workers = self.calculate_optimal_workers()
        results = []

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_file = {
                executor.submit(self.extract_single_file_metadata, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                result = future.result()
                if result:
                    results.append(result)
        
        return results

def format_file_size(size_in_bytes: int) -> str:
    """Format file size to human readable format"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_in_bytes < 1024.0:
            return f"{size_in_bytes:.2f} {unit}"
        size_in_bytes /= 1024.0
    return f"{size_in_bytes:.2f} PB"

def save_metadata_to_json(metadata_list: List[Dict], output_file: str):
    """Save metadata to JSON file"""
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, indent=2, ensure_ascii=False)



############

def _batch_get_commit_dates(self, rel_paths: List[str]) -> Dict[str, str]:
    """
    Get commit dates for multiple files in batch with multiple fallback methods.
    """
    commit_dates = {}
    
    # Method 1: Using iter_commits and stats (fastest)
    try:
        for commit in self.repo.iter_commits():
            for rel_path in rel_paths:
                if rel_path not in commit_dates:
                    try:
                        if rel_path in commit.stats.files:
                            commit_dates[rel_path] = commit.committed_datetime.isoformat()
                    except:
                        continue
    except:
        pass

    # Method 2: Try git log for files without commit dates
    remaining_paths = [path for path in rel_paths if path not in commit_dates]
    if remaining_paths:
        for rel_path in remaining_paths:
            try:
                commits = list(self.repo.iter_commits(paths=rel_path, max_count=1))
                if commits:
                    commit_dates[rel_path] = commits[0].committed_datetime.isoformat()
            except:
                pass

    # Method 3: Use git blame as last resort
    remaining_paths = [path for path in rel_paths if path not in commit_dates]
    if remaining_paths:
        for rel_path in remaining_paths:
            try:
                full_path = os.path.join(self.repo_root, rel_path)
                if os.path.exists(full_path):
                    blame = self.repo.blame('HEAD', rel_path)
                    if blame and blame[0]:
                        commit = blame[0][0]
                        commit_dates[rel_path] = commit.committed_datetime.isoformat()
            except:
                pass

    return commit_dates

def process_files_batch(self, file_paths: Optional[List[str]] = None) -> List[Dict]:
    """Process all files with optimized batch processing."""
    # Get all files if not provided
    if file_paths is None:
        file_paths = self.get_all_files()
    
    # Prepare basic file metadata first
    metadata_dict = {}
    rel_paths = []
    
    for file_path in file_paths:
        try:
            file_name = os.path.basename(file_path)
            file_stats = os.stat(file_path)
            rel_path = os.path.relpath(file_path, self.repo_root)
            
            metadata_dict[rel_path] = {
                'file_name': file_name,
                'file_path': os.path.abspath(file_path),
                'file_size': format_file_size(file_stats.st_size),
                'file_type': os.path.splitext(file_name)[1].lower().lstrip('.'),
                'relative_path': rel_path,
                'latest_commit_date': None
            }
            rel_paths.append(rel_path)
        except:
            continue

    # Get commit dates using multiple methods
    commit_dates = self._batch_get_commit_dates(rel_paths)
    
    # Update metadata with commit dates
    for rel_path, commit_date in commit_dates.items():
        if rel_path in metadata_dict:
            metadata_dict[rel_path]['latest_commit_date'] = commit_date

    return list(metadata_dict.values())

"""
Example usage:

# For Databricks /tmp directory:
repo_path = "/tmp/va.team-gov"
file_types = ['md']  # or ['.md'] - both work

extractor = GitMetadataExtractor(repo_path, file_types=file_types)
metadata = extractor.process_files_batch()

# Save to DBFS for persistence
save_metadata_to_json(metadata, '/dbfs/FileStore/metadata_results.json')

# Or save to /tmp if temporary storage is fine
save_metadata_to_json(metadata, '/tmp/metadata_results.json')
"""
