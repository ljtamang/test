import os
import git
from datetime import datetime
from typing import Dict, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

class GitMetadataExtractor:
    def __init__(self, repo_path: str, file_types: Optional[List[str]] = None):
        """
        Initialize extractor with Git repository path and optional file type filter.
        
        Parameters:
        repo_path (str): Path to Git repository root
        file_types (List[str], optional): List of file extensions to process (without dots)
                                        e.g., ['py', 'java', 'cpp']
                                        None or empty list means process all files
        """
        self.repo = git.Repo(repo_path)
        self.repo_root = repo_path
        # Set file_types to None if not specified or empty list
        self.file_types = {ext.lstrip('.').lower() for ext in file_types} if file_types and len(file_types) > 0 else None

    def is_valid_file_type(self, file_name: str) -> bool:
        """
        Check if file type should be processed based on filter.
        
        Parameters:
        file_name (str): Name of the file to check
        
        Returns:
        bool: True if file should be processed, False otherwise
        """
        # If no file types specified or empty list, process all files
        if self.file_types is None:
            return True
        file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
        return file_ext in self.file_types

    def extract_single_file_metadata(self, file_path: str) -> Optional[Dict]:
        """
        Extract metadata for a single file within the Git repository.
        
        Parameters:
        file_path (str): Full path to the file
        
        Returns:
        Dict: Metadata for the file, or None if file type should be skipped
        """
        try:
            file_name = os.path.basename(file_path)
            
            # Check file type filter
            if not self.is_valid_file_type(file_name):
                return None

            # Basic file metadata
            file_stats = os.stat(file_path)
            file_type = os.path.splitext(file_name)[1].lower().lstrip('.')
            
            # Get relative path
            rel_path = os.path.relpath(file_path, self.repo_root)
            
            # Initialize with basic info
            file_info = {
                'file_name': file_name,
                'file_path': os.path.abspath(file_path),
                'file_size': format_file_size(file_stats.st_size),
                'file_type': file_type,
                'last_modified': datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                'creation_time': datetime.fromtimestamp(file_stats.st_ctime).isoformat(),
                'relative_path': rel_path,
                'latest_commit_date': None
            }
            
            # Get latest commit date
            try:
                commits = list(self.repo.iter_commits(paths=rel_path))
                if commits:
                    file_info['latest_commit_date'] = commits[0].committed_datetime.isoformat()
            except Exception as git_error:
                print(f"Warning: Error getting commit info for {rel_path}: {git_error}")
            
            return file_info
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            return None

    def process_files_batch(self, file_paths: Optional[List[str]] = None, num_workers: Optional[int] = None) -> List[Dict]:
        """
        Process multiple files in parallel using ThreadPoolExecutor.
        
        Parameters:
        file_paths (List[str], optional): List of specific file paths to process.
                                      If None, processes all files in repo.
        num_workers (int, optional): Number of worker threads. 
                                   If None, uses min(32, os.cpu_count() * 4)
        
        Returns:
        List[Dict]: List of metadata dictionaries
        """
        # If no specific files provided, get all files from repo
        if file_paths is None:
            file_paths = []
            for root, _, files in os.walk(self.repo_root):
                if '.git' in root:  # Skip .git directory
                    continue
                for file in files:
                    file_paths.append(os.path.join(root, file))

        # Default number of workers calculation
        if num_workers is None:
            num_workers = min(32, (os.cpu_count() or 1) * 4)

        results = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_file = {
                executor.submit(self.extract_single_file_metadata, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                result = future.result()
                if result:  # Only append if result is not None
                    results.append(result)
        
        return results

def format_file_size(size_in_bytes: int) -> str:
    """Format file size to human readable format"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_in_bytes < 1024.0:
            return f"{size_in_bytes:.2f} {unit}"
        size_in_bytes /= 1024.0
    return f"{size_in_bytes:.2f} PB"

def save_metadata_to_json(metadata_list: List[Dict], output_file: str):
    """
    Save metadata to JSON file
    
    Parameters:
    metadata_list (List[Dict]): List of metadata dictionaries
    output_file (str): Path to output JSON file
    """
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, indent=2, ensure_ascii=False)
