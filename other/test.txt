from datetime import datetime
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

def process_large_repo():
    """
    Process a large repository with specific file types and exclusions.
    Only copies essential file types to worker nodes and excludes .git folder
    from file metadata processing.
    """
    
    # Initialize with specific file types to minimize data transfer
    extractor = GitCommitDateExtractor(
        repo_path="/path/to/large/repo",
        temp_dir="/mnt/fast_disk/git_temp",  # Use fast storage
        threads_per_worker=16,               # Increase parallel processing
        file_types=[                         # Only copy and process these files
            'py',    # Python files
            'java',  # Java files
            'cpp',   # C++ files
            'h',     # Header files
            'jsx',   # React files
            'ts'     # TypeScript files
        ]
    )
    
    try:
        # Get files excluding .git and other directories
        files = extractor.get_all_files(
            exclude_folders=[
                '.git',      # Git directory
                'node_modules',
                'build',
                'dist',
                'venv',
                'env',
                '__pycache__'
            ]
        )
        
        total_files = len(files)
        print(f"Found {total_files} files to process")
        
        # Process in chunks to manage memory
        chunk_size = 5000
        start_time = datetime.now()
        
        for i in range(0, total_files, chunk_size):
            chunk = files[i:i + chunk_size]
            chunk_num = i // chunk_size + 1
            total_chunks = (total_files + chunk_size - 1) // chunk_size
            
            print(f"\nProcessing chunk {chunk_num}/{total_chunks}")
            print(f"Files {i + 1} to {min(i + chunk_size, total_files)} of {total_files}")
            
            # Process current chunk with desired date information
            dates = extractor.get_commit_dates_distributed(
                chunk,
                get_created=True,          # Get file creation date
                get_content_change=True,    # Get last content modification
                num_partitions=20          # Adjust based on cluster size
            )
            
            # Process results for current chunk
            for file_path, file_dates in dates.items():
                print(f"\nFile: {file_path}")
                if 'created' in file_dates:
                    print(f"  Created: {file_dates['created']}")
                if 'latest' in file_dates:
                    print(f"  Latest commit: {file_dates['latest']}")
                if 'content_change' in file_dates:
                    print(f"  Last content change: {file_dates['content_change']}")
            
            # Show progress
            elapsed_time = (datetime.now() - start_time).total_seconds()
            files_processed = min(i + chunk_size, total_files)
            rate = files_processed / elapsed_time if elapsed_time > 0 else 0
            
            print(f"\nProgress: {(files_processed/total_files)*100:.2f}%")
            print(f"Processing rate: {rate:.2f} files/second")
            print(f"Elapsed time: {elapsed_time:.2f} seconds")
            
    finally:
        extractor.cleanup()
