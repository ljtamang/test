import os
import git
from typing import Dict, List, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

class GitMetadataExtractor:
    def __init__(self, repo_path: str, file_types: Optional[List[str]] = None):
        """
        Initialize extractor with Git repository path and optional file type filter.
        
        Parameters:
        repo_path (str): Path to Git repository root
        file_types (List[str], optional): List of file extensions to process (without dots)
        """
        self.repo = git.Repo(repo_path)
        self.repo_root = repo_path
        self.file_types = {ext.lstrip('.').lower() for ext in file_types} if file_types and len(file_types) > 0 else None

    def calculate_optimal_workers(self) -> int:
        """Calculate optimal number of worker threads based on environment."""
        try:
            from pyspark import SparkContext
            sc = SparkContext.getOrCreate()
            max_workers = sc.defaultParallelism
        except:
            max_workers = 32
        
        cpu_count = os.cpu_count() or 1
        suggested_workers = cpu_count * 4
        return min(suggested_workers, max_workers)

    def is_valid_file_type(self, file_name: str) -> bool:
        """Check if file type should be processed based on filter."""
        if self.file_types is None:
            return True
        file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
        return file_ext in self.file_types

    def extract_single_file_metadata(self, file_path: str) -> Optional[Dict]:
        """Extract metadata for a single file within the Git repository."""
        try:
            file_name = os.path.basename(file_path)
            
            if not self.is_valid_file_type(file_name):
                return None

            file_stats = os.stat(file_path)
            file_type = os.path.splitext(file_name)[1].lower().lstrip('.')
            rel_path = os.path.relpath(file_path, self.repo_root)
            
            file_info = {
                'file_name': file_name,
                'file_path': os.path.abspath(file_path),
                'file_size': format_file_size(file_stats.st_size),
                'file_type': file_type,
                'relative_path': rel_path,
                'latest_commit_date': None
            }
            
            try:
                commits = list(self.repo.iter_commits(paths=rel_path))
                if commits:
                    file_info['latest_commit_date'] = commits[0].committed_datetime.isoformat()
            except Exception as git_error:
                pass
            
            return file_info
            
        except Exception:
            return None

    def process_files_batch(self, file_paths: Optional[List[str]] = None) -> List[Dict]:
        """Process multiple files in parallel using ThreadPoolExecutor."""
        if file_paths is None:
            file_paths = []
            for root, _, files in os.walk(self.repo_root):
                if '.git' in root:
                    continue
                for file in files:
                    file_paths.append(os.path.join(root, file))

        num_workers = self.calculate_optimal_workers()
        results = []

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_file = {
                executor.submit(self.extract_single_file_metadata, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                result = future.result()
                if result:
                    results.append(result)
        
        return results

def format_file_size(size_in_bytes: int) -> str:
    """Format file size to human readable format"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_in_bytes < 1024.0:
            return f"{size_in_bytes:.2f} {unit}"
        size_in_bytes /= 1024.0
    return f"{size_in_bytes:.2f} PB"

def save_metadata_to_json(metadata_list: List[Dict], output_file: str):
    """Save metadata to JSON file"""
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, indent=2, ensure_ascii=False)

# Example usage
if __name__ == "__main__":
    repo_path = "/path/to/repo"
    file_types = ['md']
    
    extractor = GitMetadataExtractor(repo_path, file_types=file_types)
    metadata = extractor.process_files_batch()
    save_metadata_to_json(metadata, 'metadata_results.json')
