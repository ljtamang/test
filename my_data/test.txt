"""Azure Storage File Upload Module.

Provides functionality for parallel file uploads to Azure Blob Storage with progress tracking
and individual file upload timestamps.
"""

import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Union, Optional
from azure.storage.blob import BlobServiceClient
from datetime import datetime, timezone

_blob_service_client_cache: Optional[BlobServiceClient] = None

def initialize_blob_client(
    storage_account_name: str,
    storage_key: str
) -> BlobServiceClient:
    """Initialize or retrieve cached Azure Blob Service Client.
    
    Args:
        storage_account_name: Azure storage account name
        storage_key: Azure storage account access key
        
    Returns:
        BlobServiceClient: Instance for Azure Blob operations
    """
    global _blob_service_client_cache
    if _blob_service_client_cache is None:
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_key};EndpointSuffix=core.windows.net"
        _blob_service_client_cache = BlobServiceClient.from_connection_string(connection_string)
    return _blob_service_client_cache

def upload_single_blob(
    local_file_path: str,
    local_repo_path: str, 
    blob_service_client: BlobServiceClient,
    container_name: str,
    destination_folder: str,
    max_file_size: Optional[int] = None
) -> Dict[str, Optional[Union[str, datetime]]]:
    """Upload single file to Azure Storage.
    
    Args:
        local_file_path: Full path to local file
        local_repo_path: Base path of local repository
        blob_service_client: Azure blob service client
        container_name: Azure storage container name
        destination_folder: Target folder in blob storage
        max_file_size: Maximum allowed file size in bytes
        
    Returns:
        Dict containing upload status and timestamp for the file
    """
    result = {
        "file_path": local_file_path,
        "file_relative_path": None,
        "upload_status": "fail",
        "blob_path": None,
        "blob_url": None,
        "error": None,
        "upload_timestamp": None
    }
    
    try:
        if not os.path.exists(local_file_path):
            result["error"] = f"File does not exist: {local_file_path}"
            return result
            
        if max_file_size is not None:
            file_size = os.path.getsize(local_file_path)
            if file_size > max_file_size:
                result["error"] = f"File exceeds maximum size limit of {max_file_size/1024/1024:.2f}MB"
                return result
            
        file_relative_path = os.path.relpath(local_file_path, local_repo_path)
        result["file_relative_path"] = file_relative_path

        blob_path = os.path.join(destination_folder, file_relative_path)
        result["blob_path"] = blob_path

        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
        blob_client.upload_blob(local_file_path, overwrite=True)

        result["upload_status"] = "success"
        result["blob_url"] = blob_client.url
        result["upload_timestamp"] = datetime.now(timezone.utc)
    except Exception as e:
        result["error"] = str(e)
    return result

def upload_files_to_azure_storage(
    file_paths: List[str],
    local_repo_path: str,
    storage_account_name: str,
    storage_key: str,
    container_name: str,
    destination_folder: str,
    num_workers: int = 5,
    max_file_size: Optional[int] = None
) -> List[Dict[str, Optional[Union[str, datetime]]]]:
    """Upload multiple files to Azure Storage with progress tracking.
    
    Args:
        file_paths: List of file paths to upload
        local_repo_path: Base path of local repository
        storage_account_name: Azure storage account name
        storage_key: Azure storage account key
        container_name: Azure container name
        destination_folder: Destination folder in Azure Storage
        num_workers: Number of parallel upload workers
        max_file_size: Maximum allowed file size in bytes
        
    Returns:
        List of dicts with upload status and timestamp for each file
    """
    blob_service_client = initialize_blob_client(storage_account_name, storage_key)
    total_files = len(file_paths)
    completed_files = 0
    results = []

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = {
            executor.submit(
                upload_single_blob,
                file_path,
                local_repo_path,
                blob_service_client,
                container_name,
                destination_folder,
                max_file_size
            ): file_path for file_path in file_paths
        }
        
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            completed_files += 1
            
            success_count = sum(1 for r in results if r["upload_status"] == "success")
            print(f"\rProgress: {completed_files}/{total_files} files processed "
                  f"({success_count} successful, {completed_files - success_count} failed)", 
                  end="", flush=True)

    success_count = sum(1 for r in results if r["upload_status"] == "success")
    fail_count = total_files - success_count
    success_rate = (success_count / total_files) * 100 if total_files > 0 else 0
    
    print(f"\n\nUpload completed:")
    print(f"Total files: {total_files}")
    print(f"Successful: {success_count}")
    print(f"Failed: {fail_count}")
    print(f"Success rate: {success_rate:.2f}%")

    return results

if __name__ == "__main__":
    # Example usage with multiple files
    file_paths = [
        "/data/files/document1.pdf",
        "/data/files/document2.pdf",
        "/data/files/large_file.zip"
    ]
    
    results = upload_files_to_azure_storage(
        file_paths=file_paths,
        local_repo_path="/data/files",
        storage_account_name="mystorageaccount",
        storage_key="storage-key-123",
        container_name="mycontainer",
        destination_folder="uploads/2024",
        num_workers=3,
        max_file_size=5 * 1024 * 1024  # 5MB limit
    )
    
    # Print individual file results with timestamps
    for result in results:
        print(f"\nFile: {result['file_path']}")
        print(f"Status: {result['upload_status']}")
        if result['upload_status'] == 'success':
            print(f"Uploaded at: {result['upload_timestamp'].isoformat()}")
        else:
            print(f"Error: {result['error']}")

"""
Example Output:

Progress: 3/3 files processed (2 successful, 1 failed)

Upload completed:
Total files: 3
Successful: 2
Failed: 1
Success rate: 66.67%

File: /data/files/document1.pdf
Status: success
Uploaded at: 2024-01-26T14:30:25.123456+00:00

File: /data/files/document2.pdf
Status: success
Uploaded at: 2024-01-26T14:30:25.789012+00:00

File: /data/files/large_file.zip
Status: fail
Error: File exceeds maximum size limit of 5.00MB
"""
