import os
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Union, Optional
from azure.storage.blob import BlobServiceClient

_blob_service_client_cache: Optional[BlobServiceClient] = None

def initialize_blob_client(
    storage_account_name: str, 
    storage_key: str
) -> BlobServiceClient:
    """Initialize or get existing Azure Blob Service Client.
    
    Args:
        storage_account_name (str): Azure storage account name
        storage_key (str): Azure storage account access key
    
    Returns:
        BlobServiceClient: Azure blob service client instance
    """
    global _blob_service_client_cache
    if _blob_service_client_cache is None:
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_key};EndpointSuffix=core.windows.net"
        _blob_service_client_cache = BlobServiceClient.from_connection_string(connection_string)
        print("Created a new BlobServiceClient instance.")
    else:
        print("Reusing existing BlobServiceClient instance.")
    return _blob_service_client_cache

def upload_single_blob(
    local_file_path: str,
    local_repo_path: str,
    blob_service_client: BlobServiceClient,
    container_name: str,
    destination_folder: str,
    max_file_size: Optional[int] = None
) -> Dict[str, Optional[str]]:
    """Upload a single file to Azure Blob Storage.
    
    Args:
        local_file_path (str): Full path to local file
        local_repo_path (str): Base path of local repository
        blob_service_client (BlobServiceClient): Azure blob service client
        container_name (str): Azure storage container name
        destination_folder (str): Target folder in blob storage
        max_file_size (Optional[int]): Maximum allowed file size in bytes
    
    Returns:
        Dict[str, Optional[str]]: Upload status containing:
            file_path: Original file path
            relative_path: Path relative to repo
            status: Upload status ("uploaded"/"failed")
            blob_path: Path in blob storage
            blob_url: Full URL to blob
            error: Error message if failed
    """
    result = {
        "file_path": local_file_path,
        "relative_path": None,
        "status": "failed",
        "blob_path": None,
        "blob_url": None,
        "error": None
    }
    
    try:
        if not os.path.exists(local_file_path):
            result["error"] = f"File does not exist: {local_file_path}"
            return result
            
        if max_file_size is not None:
            file_size = os.path.getsize(local_file_path)
            if file_size > max_file_size:
                result["error"] = f"File exceeds maximum size limit of {max_file_size/1024/1024:.2f}MB"
                return result
            
        relative_path = os.path.relpath(local_file_path, local_repo_path)
        result["relative_path"] = relative_path

        blob_path = os.path.join(destination_folder, relative_path)
        result["blob_path"] = blob_path

        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
        blob_client.upload_blob(local_file_path, overwrite=True)

        result["status"] = "uploaded"
        result["blob_url"] = blob_client.url
    except Exception as e:
        result["error"] = str(e)
    return result

def upload_blobs_parallel(
    file_paths: List[str],
    local_repo_path: str,
    storage_account_name: str,
    storage_key: str,
    container_name: str,
    destination_folder: str,
    num_workers: int = 5,
    max_file_size: Optional[int] = None
) -> List[Dict[str, Optional[str]]]:
    """Upload multiple files to Azure Blob Storage in parallel.
    
    Args:
        file_paths (List[str]): List of file paths to upload
        local_repo_path (str): Base path of local repository
        storage_account_name (str): Azure storage account name
        storage_key (str): Azure storage account access key
        container_name (str): Azure storage container name
        destination_folder (str): Target folder in blob storage
        num_workers (int, optional): Number of parallel workers. Defaults to 5.
        max_file_size (Optional[int], optional): Maximum allowed file size in bytes. Defaults to None.
    
    Returns:
        List[Dict[str, Optional[str]]]: List of upload status dictionaries for each file
    """
    blob_service_client = initialize_blob_client(storage_account_name, storage_key)

    results = []
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [
            executor.submit(upload_single_blob, file_path, local_repo_path, blob_service_client, 
                          container_name, destination_folder, max_file_size)
            for file_path in file_paths
        ]
        for future in futures:
            results.append(future.result())

    return results

if __name__ == "__main__":
    results = upload_blobs_parallel(
        file_paths=["/dbfs/mnt/my_local_repo/file1.txt"],
        local_repo_path="/dbfs/mnt/my_local_repo",
        storage_account_name="your_storage_account_name",
        storage_key="your_storage_account_key",
        container_name="your_container_name",
        destination_folder="your_destination_folder"
    )
