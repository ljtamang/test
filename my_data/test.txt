from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import col, lit, to_timestamp
from typing import List, Dict
from datetime import datetime
import pytz

def get_standardized_timestamp() -> datetime:
   """Get current UTC timestamp for ETL operations
   
   Returns:
       datetime: Current UTC datetime for consistent ETL timestamps
   """
   return datetime.now(pytz.UTC)

def first_time_load_file_metadata_to_table(metadata_list: List[Dict]) -> None:
   """Initial load of file metadata into Delta table
   
   Args:
       metadata_list: List of dictionaries containing file metadata
           Required fields:
               - file_name (str): Name of file
               - file_relative_path (str): Relative path of file 
               - file_extension (str): File extension
               - category (str): File category
               - git_blob_hash (str): Git blob hash
               - upload_status (str): Upload status
           Optional fields:
               - upload_on (timestamp): Upload timestamp
               - blob_url (str): Blob storage URL
               - error_message (str): Error message if any
   
   Returns:
       None: Writes data to Delta table 'vfs_raw.file_metadata'
   """
   spark = SparkSession.builder.appName("FirstTimeLoadMetadata").getOrCreate()
   
   input_schema = StructType([
       StructField("file_name", StringType(), False),
       StructField("file_relative_path", StringType(), False),
       StructField("file_extension", StringType(), False),
       StructField("category", StringType(), False),
       StructField("git_blob_hash", StringType(), False),
       StructField("upload_status", StringType(), False),
       StructField("upload_on", TimestampType(), True),
       StructField("blob_url", StringType(), True),
       StructField("error_message", StringType(), True)
   ])
   
   df = spark.createDataFrame(metadata_list, schema=input_schema)
   current_time = get_standardized_timestamp()
   
   processed_df = df \
       .withColumn("etl_created_at", lit(current_time)) \
       .withColumn("etl_updated_at", lit(current_time))
   
   processed_df.write.format("delta") \
       .mode("overwrite") \
       .option("mergeSchema", "true") \
       .saveAsTable("vfs_raw.file_metadata")

def update_file_metadata_table(metadata_list: List[Dict]) -> None:
   """Update existing records in file metadata Delta table
   
   Args:
       metadata_list: List of dictionaries with updated metadata
           Required fields:
               - file_name (str): Name of file
               - file_relative_path (str): Relative path of file
               - file_extension (str): File extension  
               - category (str): File category
               - git_blob_hash (str): Git blob hash
               - upload_status (str): Upload status
           Optional fields:
               - upload_on (timestamp): Upload timestamp
               - blob_url (str): Blob storage URL
               - error_message (str): Error message if any
   
   Returns:
       None: Updates existing records in 'vfs_raw.file_metadata'
   """
   spark = SparkSession.builder.appName("UpdateMetadata").getOrCreate()
   
   input_schema = StructType([
       StructField("file_name", StringType(), False),
       StructField("file_relative_path", StringType(), False),
       StructField("file_extension", StringType(), False),
       StructField("category", StringType(), False),
       StructField("git_blob_hash", StringType(), False),
       StructField("upload_status", StringType(), False),
       StructField("upload_on", TimestampType(), True),
       StructField("blob_url", StringType(), True),
       StructField("error_message", StringType(), True)
   ])

   update_df = spark.createDataFrame(metadata_list, schema=input_schema)
   current_time = get_standardized_timestamp()
   
   processed_df = update_df \
       .withColumn("etl_updated_at", lit(current_time))

   target_df = spark.read.format("delta").table("vfs_raw.file_metadata")
   
   merge_condition = """
       target.file_name = source.file_name AND
       target.file_relative_path = source.file_relative_path
   """
   
   target_df.alias("target").merge(
       processed_df.alias("source"),
       merge_condition
   ).whenMatchedUpdate(set={
       "upload_status": "source.upload_status",
       "upload_on": "source.upload_on",
       "blob_url": "source.blob_url",
       "error_message": "source.error_message", 
       "etl_updated_at": "source.etl_updated_at"
   }).execute()
