from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import to_timestamp, current_timestamp
from typing import List, Dict

def first_time_load_metadata_to_delta_table(metadata_list: List[Dict]):
    """
    Performs the first-time load of file metadata into the vfs_raw.file_metadata Delta table.
    Uses overwrite mode to ensure the table is fresh and doesn't contain duplicates.

    Args:
        metadata_list: List of dictionaries, where each dictionary contains metadata for a file.
                      Each dictionary must have keys: file_name, file_relative_path, file_extension,
                      category, git_last_commit_date, git_blob_hash, upload_status, upload_on, blob_url, error_message.
    """
    spark = SparkSession.builder.appName("FirstTimeLoadMetadata").getOrCreate()
    
    # Define the schema explicitly with TimestampType for timestamp fields
    schema = StructType([
        StructField("file_name", StringType(), nullable=False),
        StructField("file_relative_path", StringType(), nullable=False),
        StructField("file_extension", StringType(), nullable=False),
        StructField("category", StringType(), nullable=False),
        StructField("git_last_commit_date", TimestampType(), nullable=False),
        StructField("git_blob_hash", StringType(), nullable=False),
        StructField("upload_status", StringType(), nullable=False),
        StructField("upload_on", TimestampType(), nullable=True),
        StructField("blob_url", StringType(), nullable=True),
        StructField("error_message", StringType(), nullable=True),
        StructField("etl_created_at", TimestampType(), nullable=False),
        StructField("etl_updated_at", TimestampType(), nullable=False)
    ])
    
    # Add timestamp fields to the metadata before creating DataFrame
    enriched_metadata = []
    current_time = current_timestamp().eval()
    
    for item in metadata_list:
        item_copy = item.copy()
        item_copy['etl_created_at'] = current_time
        item_copy['etl_updated_at'] = current_time
        enriched_metadata.append(item_copy)
    
    # Create DataFrame with complete schema including timestamp fields
    initial_df = spark.createDataFrame(enriched_metadata, schema=schema)
    
    # Insert all records into the Delta table in overwrite mode
    initial_df.write.format("delta") \
        .mode("overwrite") \
        .saveAsTable("vfs_raw.file_metadata")
    
    print("First-time load of metadata into Delta table completed successfully.")
