# VFS/notebooks/ingestion.ipynb
# Databricks notebook source

# MAGIC %md
# MAGIC ## Simple Ingestion - Direct Config Access

# COMMAND ----------

# âœ… One line setup
from vfs_setup import PROJECT_ROOT

print(f"ğŸ“ Project Root: {PROJECT_ROOT}")

# COMMAND ----------

# âœ… Direct access to config
from config import load_config, get_database_name, get_batch_size

# Load full config
config = load_config("dev")
print(f"ğŸŒ Environment: {config['environment_name']}")
print(f"ğŸ—„ï¸  Database: {config['database']['name']}")

# Or get specific values directly
database_name = get_database_name("dev")
batch_size = get_batch_size("dev")

print(f"ğŸ“Š Batch Size: {batch_size}")
print(f"ğŸ—„ï¸  Database: {database_name}")

# COMMAND ----------

# âœ… Use config values directly in your code
input_path = f"{config['paths']['input']}/customer_data.csv"
output_table = config['database']['name'] + ".processed_customers"

print(f"ğŸ“„ Input: {input_path}")
print(f"ğŸ“‹ Output Table: {output_table}")

# Your data processing code here...
# df = spark.read.csv(input_path)
# df.write.saveAsTable(output_table)

# COMMAND ----------

# âœ… Switch environments easily
prod_config = load_config("prod")
if prod_config:
    print(f"ğŸš€ PROD Database: {prod_config['database']['name']}")
    print(f"ğŸ“Š PROD Batch Size: {prod_config['processing']['batch_size']}")
