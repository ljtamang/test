from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, monotonically_increasing_id
from typing import Dict

def remove_duplicates_by_hash(table_name: str) -> Dict[str, int]:
    """
    Removes duplicate rows from a Delta table based on `git_blob_hash`, keeping only the row with the latest `etl_updated_at`.

    Args:
        table_name (str): The name of the Delta table to remove duplicates from.

    Returns:
        Dict[str, int]: A dictionary containing:
            - "total_records": Total records in the table before deletion.
            - "deleted_records": Number of duplicate records deleted.
    """
    spark = SparkSession.builder.getOrCreate()
    
    # Read the table into a DataFrame
    df = spark.table(table_name)
    
    # Add a unique identifier column
    df_with_id = df.withColumn("unique_id", monotonically_increasing_id())
    
    # Define a window specification
    window_spec = Window.partitionBy("git_blob_hash").orderBy(col("etl_updated_at").desc())
    
    # Add a row number column to each row within the window
    df_with_row_num = df_with_id.withColumn("row_num", row_number().over(window_spec))
    
    # Create a temporary view for the DataFrame with row numbers
    temp_view_name = "file_metadata_with_row_num"
    df_with_row_num.createOrReplaceTempView(temp_view_name)
    
    # Delete rows where row_num is greater than 1 (i.e., duplicates)
    delete_query = f"""
    DELETE FROM {table_name}
    WHERE unique_id IN (
        SELECT unique_id
        FROM {temp_view_name}
        WHERE row_num > 1
    )
    """
    
    # Get the initial count of records
    initial_count = df.count()
    
    # Execute the delete query
    spark.sql(delete_query)
    
    # Get the final count of records
    final_count = spark.table(table_name).count()
    
    # Drop the temporary view
    spark.catalog.dropTempView(temp_view_name)
    
    return {
        "total_records": initial_count,
        "deleted_records": initial_count - final_count
    }
