def remove_duplicates_by_hash(table_name: str) -> Dict[str, int]:
    spark = SparkSession.builder.getOrCreate()
    
    initial_count = spark.table(table_name).count()
    
    # Use monotonically_increasing_id() to break ties
    spark.sql(f"""
        CREATE OR REPLACE TEMPORARY VIEW temp_ranked AS
        SELECT *,
            ROW_NUMBER() OVER (
                PARTITION BY git_blob_hash 
                ORDER BY etl_updated_at DESC, monotonically_increasing_id()
            ) as rn
        FROM {table_name}
    """)
    
    spark.sql(f"""
        DELETE FROM {table_name}
        WHERE git_blob_hash IN (
            SELECT git_blob_hash 
            FROM temp_ranked 
            WHERE rn > 1
        )
    """)
    
    final_count = spark.table(table_name).count()
    
    return {
        "total_records": initial_count,
        "deleted_records": initial_count - final_count
    }
