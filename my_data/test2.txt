from pyspark.sql import SparkSession
from typing import Dict

def remove_duplicates_by_hash(table_name: str) -> Dict[str, int]:
   """
   Removes duplicate records from a Delta table based on git_blob_hash, keeping one record with latest etl_updated_at.
   For records with identical etl_updated_at, keeps one record randomly.

   Args:
       table_name (str): Name of the Delta table

   Returns:
       Dict[str, int]: Contains:
           - total_records (int): Initial record count
           - deleted_records (int): Number of records deleted
   """
   delete_query = f"""
   DELETE FROM {table_name} 
   WHERE git_blob_hash IN (
       SELECT t.git_blob_hash
       FROM (
           SELECT ARBITRARY(*) as arbitrary_record,
               git_blob_hash
           FROM {table_name}
           GROUP BY git_blob_hash
           HAVING COUNT(*) > 1
       ) t
   )
   AND (git_blob_hash, etl_updated_at) NOT IN (
       SELECT git_blob_hash, MAX(etl_updated_at)
       FROM {table_name}
       GROUP BY git_blob_hash
   )
   """
   
   spark = SparkSession.builder.getOrCreate()
   initial_count = spark.table(table_name).count()
   spark.sql(delete_query)
   final_count = spark.table(table_name).count()
   
   return {
       "total_records": initial_count,
       "deleted_records": initial_count - final_count
   }
