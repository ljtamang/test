 from pyspark.sql import SparkSession

def deduplicate_file_metadata() -> dict:
    spark = SparkSession.builder.getOrCreate()
    
    # Create temp view with row numbers for each git_blob_hash
    spark.sql("""
        CREATE OR REPLACE TEMPORARY VIEW temp_ranked AS
        SELECT *,
            ROW_NUMBER() OVER (
                PARTITION BY git_blob_hash 
                ORDER BY etl_updated_at DESC, rand()
            ) as rn
        FROM vfs_raw.file_metadata
    """)
    
    # Get initial count
    initial_count = spark.table("vfs_raw.file_metadata").count()
    
    # Delete all rows except the ones with rn = 1
    spark.sql("""
        DELETE FROM vfs_raw.file_metadata 
        WHERE (git_blob_hash) IN (
            SELECT git_blob_hash 
            FROM temp_ranked 
            WHERE rn > 1
        )
    """)
    
    final_count = spark.table("vfs_raw.file_metadata").count()
    
    return {
        "total_records": initial_count,
        "deleted_records": initial_count - final_count
    }
