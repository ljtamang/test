def remove_duplicates_by_hash(table_name: str) -> dict:
   """
   Removes duplicate records keeping the latest by etl_updated_at for each git_blob_hash.
   Returns statistics and deleted hashes.
   """
   # Query to identify and delete duplicates
   delete_query = f"""
   DELETE FROM {table_name}
   WHERE (git_blob_hash, etl_updated_at) IN (
       SELECT git_blob_hash, etl_updated_at
       FROM (
           SELECT *,
               ROW_NUMBER() OVER (
                   PARTITION BY git_blob_hash
                   ORDER BY etl_updated_at DESC
               ) AS row_num
           FROM {table_name}
       )
       WHERE row_num > 1
   )
   """
   
   spark = SparkSession.builder.getOrCreate()
   initial_count = spark.table(table_name).count()
   
   # Execute deletion
   spark.sql(delete_query)
   
   final_count = spark.table(table_name).count()
   deleted_count = initial_count - final_count
   
   return {
       "total_records": initial_count,
       "deleted_records": deleted_count
   }
