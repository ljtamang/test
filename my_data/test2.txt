from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col

def remove_duplicates_by_hash(table_name: str) -> Dict[str, int]:
    spark = SparkSession.builder.getOrCreate()
    
    # Read the table into a DataFrame
    df = spark.table(table_name)
    
    # Define a window specification
    window_spec = Window.partitionBy("git_blob_hash").orderBy(col("etl_updated_at").desc())
    
    # Add a row number column to each row within the window
    df_with_row_num = df.withColumn("row_num", row_number().over(window_spec))
    
    # Create a temporary view for the DataFrame with row numbers
    temp_view_name = f"{table_name}_with_row_num"
    df_with_row_num.createOrReplaceTempView(temp_view_name)
    
    # Delete rows where row_num is greater than 1 (i.e., duplicates)
    delete_query = f"""
    DELETE FROM {table_name}
    WHERE (git_blob_hash, etl_updated_at) IN (
        SELECT git_blob_hash, etl_updated_at
        FROM {temp_view_name}
        WHERE row_num > 1
    )
    """
    
    # Get the initial count of records
    initial_count = df.count()
    
    # Execute the delete query
    spark.sql(delete_query)
    
    # Get the final count of records
    final_count = spark.table(table_name).count()
    
    # Drop the temporary view
    spark.catalog.dropTempView(temp_view_name)
    
    return {
        "total_records": initial_count,
        "deleted_records": initial_count - final_count
    }
