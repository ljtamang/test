# VFS/notebooks/ingestion.ipynb
# Databricks notebook source

# MAGIC %md
# MAGIC ## Simple Ingestion - Direct Config Access

# COMMAND ----------

# ✅ One line setup
from vfs_setup import PROJECT_ROOT

print(f"📁 Project Root: {PROJECT_ROOT}")

# COMMAND ----------

# ✅ Direct access to config
from config import load_config, get_database_name, get_batch_size

# Load full config
config = load_config("dev")
print(f"🌍 Environment: {config['environment_name']}")
print(f"🗄️  Database: {config['database']['name']}")

# Or get specific values directly
database_name = get_database_name("dev")
batch_size = get_batch_size("dev")

print(f"📊 Batch Size: {batch_size}")
print(f"🗄️  Database: {database_name}")

# COMMAND ----------

# ✅ Use config values directly in your code
input_path = f"{config['paths']['input']}/customer_data.csv"
output_table = config['database']['name'] + ".processed_customers"

print(f"📄 Input: {input_path}")
print(f"📋 Output Table: {output_table}")

# Your data processing code here...
# df = spark.read.csv(input_path)
# df.write.saveAsTable(output_table)

# COMMAND ----------

# ✅ Switch environments easily
prod_config = load_config("prod")
if prod_config:
    print(f"🚀 PROD Database: {prod_config['database']['name']}")
    print(f"📊 PROD Batch Size: {prod_config['processing']['batch_size']}")
